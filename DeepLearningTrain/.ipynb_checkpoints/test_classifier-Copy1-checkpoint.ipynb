{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.导入数据集预处理、特征工程和模型训练所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6ff77d563835>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "#from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../data/database/texts.db\")\n",
    "\n",
    "print ('Opened database successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 将词、诗、文言文、期刊和新闻的数据加入data列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add successfully\n"
     ]
    }
   ],
   "source": [
    "c = conn.cursor()\n",
    "\n",
    "ci_data=[]\n",
    "cursor = c.execute(\"SELECT * from ci order by id asc limit 18000\")\n",
    "#cursor = c.execute(\"SELECT * from ci order by id\")\n",
    "for row in cursor:\n",
    "    ci_data.append(row)\n",
    "#print(\"读取的词数据样例：\")\n",
    "#print(ci_data[-2:])\n",
    "\n",
    "poet_data = []\n",
    "cursor=c.execute(\"SELECT * FROM poet order by id asc limit 18000\")\n",
    "#cursor = c.execute(\"SELECT * from poet order by id\")\n",
    "for row in cursor:\n",
    "    poet_data.append(row)\n",
    "#print(\"\\n读取的诗数据样例：\")\n",
    "#print(poet_data[-2:])\n",
    "\n",
    "classical_data = []\n",
    "cursor=c.execute(\"SELECT * FROM classical order by id asc limit 18000\")\n",
    "#cursor = c.execute(\"SELECT * from classical order by id\")\n",
    "for row in cursor:\n",
    "    classical_data.append(row)\n",
    "#print(\"\\n读取的文言文数据样例：\")\n",
    "#print(classical_data[-2:])\n",
    "\n",
    "journal_data = []\n",
    "cursor=c.execute(\"SELECT * FROM journal order by id asc limit 18000\")\n",
    "#cursor = c.execute(\"SELECT * from journal order by id\")\n",
    "for row in cursor:\n",
    "    journal_data.append(row)\n",
    "#print(\"\\n读取的期刊数据样例：\")\n",
    "#print(journal_data[:2])\n",
    "    \n",
    "news_data = []\n",
    "cursor=c.execute(\"SELECT * FROM news order by id asc limit 18000\")\n",
    "#cursor=c.execute(\"SELECT * FROM news\")\n",
    "for row in cursor:\n",
    "    news_data.append(row)\n",
    "#print(\"\\n读取的新闻数据样例：\")\n",
    "#print(news_data[:2])\n",
    "\n",
    "print(\"add successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.1 Basic方案：对原文本拆分成单字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40002209, '中 兴 曾 作 故 人 看 ， 抗 节 唯 怜 七 里 滩 。   枯 枿 卧 沙 疑 野 艇 ， 丛 篁 生 岸 忆 长 竿 。   天 边 旧 迹 星 辰 动 ， 江 上 余 基 水 石 寒 。   应 笑 渭 滨 周 吕 望 ， 白 头 因 猎 从 和 銮 。  ', 'poet', 'd1fcb4d4-c756-418d-927d-b77c92901a98']\n"
     ]
    }
   ],
   "source": [
    "data = ci_data[:4000] + poet_data[:4000] + classical_data[:4000] + journal_data[:4000] + news_data[:4000]\n",
    "data2 = ci_data[4000:8000] + poet_data[4000:8000] + classical_data[4000:8000] + journal_data[4000:8000] + news_data[4000:8000]\n",
    "data3 = ci_data[8000:12000] + poet_data[8000:12000] + classical_data[8000:12000] + journal_data[8000:12000] + news_data[8000:12000]\n",
    "data4 = ci_data[12000:16000] + poet_data[12000:16000] + classical_data[12000:16000] + journal_data[12000:16000] + news_data[12000:16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_spaces(data):\n",
    "    data_list = []\n",
    "    for line in data:\n",
    "        data_list.append(list(line))\n",
    "\n",
    "    # print(data_list[6000])\n",
    "    for item in data_list:\n",
    "        item[1] = ' '.join(list(item[1]))\n",
    "    # print(data_list[6000])\n",
    "    # print(len(data_list))\n",
    "    return data_list\n",
    "\n",
    "data_list = split_spaces(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.2  建立拆分训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['气 和 玉 烛 ， 睿 化 著 鸿 明 。   缇 管 一 阳 生 。   郊 盛 礼 燔 柴 毕 ， 旋 轸 凤 凰 城 。   森 罗 仪 卫 振 华 缨 。   载 路 溢 欢 声 。   皇 图 大 业 超 前 古 ， 垂 象 泰 阶 平 。   岁 时 丰 衍 ， 九 土 乐 升 平 。   睹 寰 海 澄 清 。   道 高 尧 舜 垂 衣 治 ， 日 月 并 文 明 。   嘉 禾 甘 露 登 歌 荐 ， 云 物 焕 祥 经 。   兢 兢 惕 惕 持 谦 德 ， 未 许 禅 云 亭 。', '严 夜 警 ， 铜 莲 漏 迟 迟 。   清 禁 肃 ， 森 陛 戟 ， 羽 卫 俨 皇 闱 。   角 声 励 ， 钲 鼓 攸 宜 。   金 管 成 雅 奏 ， 逐 吹 逶 迤 。   荐 苍 璧 ， 郊 祀 神 祗 。   属 景 运 纯 禧 。   京 坻 丰 衍 ， 群 材 乐 育 ， 诸 侯 述 职 ， 盛 德 服 蛮 夷 。   殊 祥 萃 ， 九 苞 丹 凤 来 仪 。   膏 露 降 ， 和 气 洽 ， 三 秀 焕 灵 芝 。   鸿 猷 播 ， 史 册 相 辉 。   张 四 维 。   卜 世 永 固 丕 基 。   敷 玄 化 ， 荡 荡 无 为 。   合 尧 舜 文 思 。   混 并 寰 宇 ， 休 牛 归 马 ， 销 金 偃 革 ， 蹈 咏 庆 昌 期 。', '承 宝 运 ， 驯 致 隆 平 。   鸿 庆 被 寰 瀛 。   时 清 俗 阜 ， 治 定 功 成 。   遐 迩 咏 由 庚 。   严 郊 祀 ， 文 物 声 明 。   会 天 正 、 星 拱 奏 严 更 。   布 羽 仪 簪 缨 。   宸 心 虔 洁 ， 明 德 播 惟 馨 。   动 苍 冥 。   神 降 享 精 诚 。   燔 柴 半 ， 万 乘 移 天 仗 ， 肃 銮 辂 旋 衡 。   千 官 云 拥 ， 群 后 葵 倾 。   玉 帛 旅 明 庭 。   韶 荐 ， 金 奏 谐 声 。   集 休 亨 。   皇 泽 浃 黎 庶 ， 普 率 洽 恩 荣 。   仰 钦 元 后 ， 睿 圣 贯 三 灵 。   万 邦 宁 。   景 贶 福 千 龄 。']\n",
      "['modern', 'modern', 'modern']\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "train_target=[]\n",
    "for i in range(0,len(data_list)):\n",
    "    train_data.append(data_list[i][1])\n",
    "    train_target.append(data_list[i][2])\n",
    "print(train_data[:3])\n",
    "print(train_target[15000:15003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集和训练集的切分比例为0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_target,test_size=0.3,random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.3 Pro: 清理时去掉符号（利用正则表达式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除符号成功：\n",
      "样例：【中 兴 曾 作 故 人 看 ， 抗 节 唯 怜 七 里 滩 。   枯 枿 卧 沙 疑 野 艇 ， 丛 篁 生 岸 忆 长 竿 。   天 边 旧 迹 星 辰 动 ， 江 上 余 基 水 石 寒 。   应 笑 渭 滨 周 吕 望 ， 白 头 因 猎 从 和 銮 。  】=========>【中 兴 曾 作 故 人 看 抗 节 唯 怜 七 里 滩 枯 枿 卧 沙 疑 野 艇 丛 篁 生 岸 忆 长 竿 天 边 旧 迹 星 辰 动 江 上 余 基 水 石 寒 应 笑 渭 滨 周 吕 望 白 头 因 猎 从 和 銮】\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train_data_pro = []\n",
    "for paragraph in train_data:\n",
    "    train_data_pro.append(' '.join(list((re.sub(\"[\\s+\\.\\!\\/_,$%^*)(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）“”]\", \"\",paragraph)))))\n",
    "\n",
    "\n",
    "print(\"去除符号成功：\\n样例：【\"+train_data[6000]+\"】=========>【\"+train_data_pro[6000]+\"】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['客 有 通 元 先 生 好 求 古 迹 为 余 知 书 启 之 发 源 审 以 臧 否 曰 ： 余 不 敏 何 足 以 知 之 今 率 以 见 闻 随 纪 年 代 考 究 兴 亡 其 可 为 元 龟 者 举 而 叙 之 古 者 画 卦 立 象 造 字 设 教 爱 置 形 象 肇 乎 仓 史 仰 观 俯 察 鸟 迹 垂 文 至 于 唐 虞 焕 乎 文 章 畅 于 夏 殷 备 乎 秦 汉 洎 周 宣 王 史 史 籀 循 科 斗 之 书 采 仓 颉 古 文 综 其 遗 美 别 署 新 意 号 曰 籀 文 或 谓 大 篆 秦 丞 相 李 斯 改 省 籀 文 适 时 简 要 号 曰 小 篆 善 而 行 之 其 仓 颉 象 形 传 诸 典 策 世 绝 其 迹 无 得 而 称 其 籀 文 小 篆 自 周 秦 以 来 犹 如 参 用 未 之 废 黜 或 刻 以 符 玺 或 铭 于 鼎 钟 或 书 之 旌 钺 往 往 人 间 时 有 见 者 夫 言 篆 者 传 也 书 者 如 也 述 事 契 誓 者 也 字 者 孳 也 孳 乳 浸 多 者 也 而 根 之 所 由 其 来 远 矣', '侍 坐 于 君 子 君 子 欠 伸 撰 杖 履 视 日 蚤 莫 侍 坐 者 请 出 矣 侍 坐 于 君 子 君 子 问 更 端 则 起 而 对 侍 坐 于 君 子 若 有 告 者 曰 ： 少 间 愿 有 复 也 则 左 右 屏 而 待', '圉 孙 疑 作 围 亦 可 通 犯 应 从 于 作 范 钤 制 之 意 余 则 疑 圉 未 必 讹 因 圉 字 本 具 外 围 之 义 否 则 或 为 固 即 箍 之 借 总 之 无 论 为 围 为 固 都 系 名 词 似 用 金 类 制 成 故 下 文 言 融 其 两 端 若 用 木 制 不 可 融 也 融 即 镕 依 此 解 释 则 以 束 轮 三 字 文 义 自 通 不 能 依 孙 于 以 下 增 木 字 又 于 谓 围 范 即 范 围 亦 不 合 因 此 两 字 之 上 一 字 应 为 名 词 下 一 字 应 为 动 词 非 两 字 平 举 也']\n"
     ]
    }
   ],
   "source": [
    "x_train_pro, x_test_pro, y_train, y_test = train_test_split(train_data_pro, train_target,test_size=0.3,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer(analyzer='char')\n",
    "#vector_pro = cv.fit_transform(x_train_pro)\n",
    "#vector_pro.todense()\n",
    "vectorizer_pro = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_pro.fit(x_train_pro)\n",
    "print(x_train_pro[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"去除符号后的训练结果：\")\n",
    "train_NBayes(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_TfIdf(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_nGram(x_train_pro, x_test_pro, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.4 Prop: 头尾加上`<start>` `<end>` 标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 中 兴 曾 作 故 人 看 抗 节 唯 怜 七 里 滩 枯 枿 卧 沙 疑 野 艇 丛 篁 生 岸 忆 长 竿 天 边 旧 迹 星 辰 动 江 上 余 基 水 石 寒 应 笑 渭 滨 周 吕 望 白 头 因 猎 从 和 銮 <end>\n"
     ]
    }
   ],
   "source": [
    "train_data_prop = []\n",
    "for paragraph in train_data_pro:\n",
    "    train_data_prop.append(\"<start> \"+paragraph+\" <end>\")\n",
    "\n",
    "print(train_data_prop[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> １ ３ 日 福 建 省 委 书 记 孙 春 兰 在 福 州 会 见 了 日 本 驻 华 大 使 丹 羽 宇 一 郎 Ｋ 锎 豪 妓 担 丹 羽 宇 一 郎 大 使 是 中 国 人 民 的 老 朋 友 多 年 来 为 中 日 友 好 及 中 日 经 贸 往 来 作 出 了 积 极 贡 献 我 们 对 大 使 一 行 来 闽 访 问 表 示 热 烈 欢 迎 Ｔ 诩 蛞 地 介 绍 了 福 建 省 情 后 孙 春 兰 说 福 建 历 来 重 视 发 展 与 日 本 的 友 好 关 系 积 极 致 力 于 推 动 双 方 提 升 交 流 合 作 水 平 多 年 来 两 地 经 贸 往 来 频 繁 在 电 子 石 材 制 陶 食 品 加 工 旅 游 等 领 域 都 有 许 多 合 作 项 目 取 得 了 良 好 成 效 ； 交 流 交 往 密 切 福 建 与 日 本 长 崎 县 冲 绳 县 建 立 了 友 好 关 系 还 有 ６ 个 市 也 与 日 本 的 城 市 建 立 友 好 关 系 双 方 友 好 交 往 从 政 府 到 民 间 从 人 文 到 经 济 涉 及 各 个 领 域 去 年 日 本 遭 遇 大 地 震 之 时 福 建 派 出 代 表 团 到 日 本 表 示 慰 问 积 极 为 日 本 灾 后 重 建 尽 绵 薄 之 力 她 指 出 今 年 是 中 日 邦 交 正 常 化 ４ ０ 周 年 两 国 把 今 年 确 定 为 中 日 国 民 交 流 友 好 年 福 建 和 日 本 交 流 合 作 将 迎 来 难 得 机 遇 希 望 以 大 使 此 次 访 问 为 契 机 双 方 进 一 步 增 进 相 互 了 解 加 强 经 贸 合 作 促 进 相 互 投 资 深 化 友 好 交 往 共 同 推 动 两 地 友 好 关 系 不 断 向 前 发 展 ５ び 鹩 钜 焕 筛 行 话 括 福 建 人 民 在 内 的 许 多 中 国 朋 友 给 予 灾 后 重 建 的 无 私 援 助 感 谢 福 建 省 给 予 在 闽 日 本 企 业 和 日 籍 人 士 工 作 生 活 的 关 心 照 顾 他 说 今 年 是 中 日 邦 交 正 常 化 ４ ０ 周 年 两 国 将 开 展 涉 及 各 个 领 域 的 交 流 活 动 希 望 借 开 展 活 动 之 机 加 强 两 地 经 贸 文 化 体 育 等 方 方 面 面 的 交 流 合 作 进 一 步 深 化 双 方 友 好 关 系 Ｊ × 斓 家 端 瑜 倪 岳 峰 参 加 会 见 在 座 的 还 有 随 同 大 使 来 访 的 日 本 驻 广 州 领 事 馆 总 领 事 中 原 邦 之 日 本 在 华 投 资 的 企 业 代 表 记 者 兰 锋 Ｗ 髡 撸 豪 挤 妗 ɡ 丛 矗 焊 ＝ ㄈ 毡 ǎ <end>\n"
     ]
    }
   ],
   "source": [
    "x_train_prop, x_test_prop, y_train, y_test = train_test_split(train_data_prop, train_target,test_size=0.3,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "vector_prop = cv.fit_transform(x_train_prop)\n",
    "vector_prop.todense()\n",
    "vectorizer_prop = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_prop.fit(x_train_prop)\n",
    "print(x_train_prop[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"加上标记后的训练结果：\")\n",
    "train_NBayes(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_TfIdf(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_nGram(x_train_prop, x_test_prop, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10002589, '\\t遵彼汝坟，伐其条枚。未见君子，惄如调饥。\\n', 'classical', 'c_list_10.txt'), (10002590, '\\t遵彼汝坟，伐其条肄。既见君子，不我遐弃。\\n', 'classical', 'c_list_10.txt')]\n"
     ]
    }
   ],
   "source": [
    "print(data[10000:10002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、nn.Embedding\n",
    ">pytorch的模型提供了Embedding模型用于实现词嵌入过程Embedding层中的权重用于随机初始化词的向量，权重参数在后续的训练中会被不断调整，并被优化。\n",
    "\n",
    "## （1）模型的创建方法：\n",
    "### embeding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "- vocab_size 表示字典的大小\n",
    "- embedding_dim 词嵌入的维度数量，通常设置远小于字典大小，60-300之间通常可满足需要\n",
    "\n",
    "## （2）使用：\n",
    "### embeded = embeding(input)\n",
    "\n",
    "- input 需要嵌入的句子，可为任意维度。单个句子表示为token的索引列表，如[283, 4092, 1, ]\n",
    "- output 数据的嵌入表示，shape=[*, embedding_dim]，*为input的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、nn.RNN\n",
    " RNN单元还有一些变体，主要是单元内部的激活函数不同或数据使用了不同计算。\n",
    " RNN每个单元存在输入x与上一时刻的隐层状态h，输出有y与当前时刻的隐层状态。\n",
    " 对RNN单元的改进有LSTM和GRU，这三种类型的模型的输入数据都需要3D的tensor\n",
    "\n",
    "- 使用时设置b atch_first为true时，输入数据的shape为[batch，seq_length, input_dim]\n",
    "   - 第一维为batch的数量不使用时设置为1\n",
    "   - 第二维序列的长度\n",
    "   - 第三维为输入的维度，通常为词嵌入的维度。\n",
    "- rnn = RNN(input_dim, hidden_dim, num_layers=1, batch_first, bidirectional)*\n",
    "\n",
    "参数|含义\n",
    "--|:--\n",
    "input_dim|输入token的特征数量，使用embeding时为嵌入的维度\n",
    "hidden_dim|隐层的单元数，决定RNN的输出长度\n",
    "num_layers|层数\n",
    "batch_frist|第一维为batch，反之第一堆为seq_len，默认为False\n",
    "bidirectional|是否为双向RNN，默认为False\n",
    "\n",
    "- output, hidden = rnn(input, hidden)\n",
    "\n",
    "参数|含义\n",
    "--|:--\n",
    "input|一批输入数据，shape为[batch, seq_len, input_dim]\n",
    "hidden|上一时刻的隐层状态，shape为[num_layers * num_directions, batch, hidden_dim]\n",
    "output|当前时刻的输出，shape为[batch, seq_len, num_directions*hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_of_class, weights=None, rnn_type=\"RNN\"):\n",
    "        super(TextRNN, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_of_class = num_of_class\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        if weights is not None:\n",
    "            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, _weight=weights)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        if rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "            self.hidden2label = nn.Linear(hidden_size, num_of_class)\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "            self.hidden2label = nn.Linear(hidden_size*2, num_of_class)\n",
    "\n",
    "    def forward(self, input_sents):\n",
    "        # input_sents (batch_size, seq_len)\n",
    "        batch_size, seq_len = input_sents.shape\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        embed_out = self.embed(input_sents)\n",
    "\n",
    "        if self.rnn_type == \"RNN\":\n",
    "            h0 = torch.randn(1, batch_size, self.hidden_size)\n",
    "            _, hn = self.rnn(embed_out, h0)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            h0, c0 = torch.randn(2, batch_size, self.hidden_size), torch.randn(2, batch_size, self.hidden_size)\n",
    "            output, (hn, _) = self.lstm(embed_out, (h0, c0))\n",
    "\n",
    "        logits = self.hidden2label(hn).squeeze(0)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(text_field.vocab), 300)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=300,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,# 第一个维度设为 batch, 即:(batch_size, seq_length, embedding_dim)\n",
    "                            bidirectional=True) # 是否用双向\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 1)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main\n",
    "from torch import optim\n",
    "import torch\n",
    "from models import TextRNN, TextCNN\n",
    "from dataloader_bytorchtext import dataset2dataloader\n",
    "from dataloader_byhand import make_dataloader\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_names = [\"LSTM\", \"RNN\", \"CNN\"]  # 彩蛋：按过拟合难度排序，由难到易\n",
    "    learning_rate = 0.001\n",
    "    epoch_num = 500\n",
    "    num_of_class = 5\n",
    "    load_data_by_torchtext = True\n",
    "\n",
    "    if load_data_by_torchtext:\n",
    "        train_iter, val_iter, word_vectors = dataset2dataloader(batch_size=100, debug=True)\n",
    "    else:\n",
    "        train_iter, val_iter, word_vectors, X_lang = make_dataloader(batch_size=100, debug=True)\n",
    "\n",
    "    for model_name in model_names[-1:]:\n",
    "        if model_name == \"RNN\":\n",
    "            model = TextRNN(vocab_size=len(word_vectors), embedding_dim=50, hidden_size=128, num_of_class=num_of_class, weights=word_vectors)\n",
    "        elif model_name == \"CNN\":\n",
    "            model = TextCNN(vocab_size=len(word_vectors), embedding_dim=50, num_of_class=num_of_class, embedding_vectors=word_vectors)\n",
    "        elif model_name == \"LSTM\":\n",
    "            model = TextRNN(vocab_size=len(word_vectors), embedding_dim=50, hidden_size=128, num_of_class=num_of_class, weights=word_vectors, rnn_type=\"LSTM\")\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epoch_num):\n",
    "            model.train()  # 包含dropout或者BN的模型需要指定\n",
    "            for i, batch in enumerate(train_iter):\n",
    "                if load_data_by_torchtext:\n",
    "                    x, y = batch.sent.t(), batch.label\n",
    "                else:\n",
    "                    x, y, lens = batch\n",
    "                logits = model(x)\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fun(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            model.eval()\n",
    "            train_accs = []\n",
    "            for i, batch in enumerate(train_iter):\n",
    "                if load_data_by_torchtext:\n",
    "                    x, y = batch.sent.t(), batch.label\n",
    "                else:\n",
    "                    x, y, lens = batch\n",
    "                _, y_pre = torch.max(logits, -1)\n",
    "                acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "                train_accs.append(acc)\n",
    "            train_acc = np.array(train_accs).mean()\n",
    "\n",
    "            val_accs = []\n",
    "            for i, batch in enumerate(val_iter):\n",
    "                if load_data_by_torchtext:\n",
    "                    x, y = batch.sent.t(), batch.label\n",
    "                else:\n",
    "                    x, y, lens = batch\n",
    "                logits = model(x)\n",
    "                _, y_pre = torch.max(logits, -1)\n",
    "                acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "                val_accs.append(acc)\n",
    "            val_acc = np.array(val_accs).mean()\n",
    "            print(\"epoch %d train acc:%.2f, val acc:%.2f\" % (epoch, train_acc, val_acc))\n",
    "            if train_acc >= 0.99:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 将模型保存至本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"model_NB.pickle\", \"wb\")\n",
    "# 把模型写入到文件中\n",
    "pickle.dump(model_NB, file)\n",
    "# 关闭文件\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
