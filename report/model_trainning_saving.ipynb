{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练与保存\n",
    "\n",
    "准备工作：导入训练模型和导出模型所需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "import pandas,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅰ 连接数据库，读入数据。\n",
    "\n",
    "说明：由于五类文本中【词】的数量最少，不足20000条，为保证输入类型的平衡性，因此读入阈值受【词】的数量限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n",
      "**读取的词数据样例：\n",
      "[('50021047', '楼台里，春风淡荡。  ', 'ci', 21047), ('50021048', '乍卷珠帘新燕入。  ', 'ci', 21048)]\n",
      "\n",
      "**读取的诗数据样例：\n",
      "[(40020639, '白团扇子白纱衣，怕见萤光作火吹。 屈指西风明日是，会将束缊乞怜时。 ', 'poet', 'c69b266a-958f-4a6f-ba1a-ee239d76ecf1'), (40020640, '一叶梧桐一片云，秋风江上日纷纷。 鸦来鸦去吾何预，不向声尘著意闻。 ', 'poet', 'cd2f6d52-e6ce-4fd3-875a-981056a4d579')]\n",
      "\n",
      "**读取的文言文数据样例：\n",
      "[('10023324', '\\u3000\\u3000帝曰：願聞其異狀也。岐伯曰：陽者天氣也，主外；陰者地氣也，主內。故陽道實，陰道虛。故犯賊風虛邪者陽受之，食飲不節，起居不時者，陰受之。陽受之則入六腑，陰受之則入五臟。入六腑則身熱不時臥，上為喘呼；入五臟則瞋滿閉塞，下為飧泄，久為腸澼。故喉主天氣，咽主地氣。故陽受風氣，陰受濕氣。\\u3000\\u3000故陰氣從足上行至頭，而下行循臂至指端；陽氣從手上行至頭，而下行至足。故曰陽病者上行極而下，陰病者下行極而上。故傷於風者上先受之，傷於濕者，下先受之。\\n', 'classical', 'c_list_111.txt'), ('10023325', '\\u3000\\u3000帝曰：脾病而四肢不用何也？岐伯曰：四肢皆稟氣於胃而不得至經，必因於脾乃得稟也。今脾病不能為胃行其津液，四肢不得稟水谷氣，氣日以衰，脈道不利，筋骨肌內，皆無氣以生，故不用焉。\\n', 'classical', 'c_list_111.txt')]\n",
      "\n",
      "**读取的期刊数据样例：\n",
      "[(20000001, '作者程苏东, 北京大学中文系讲师 (北京100871) 。\\n', 'modern', 'j_list_1.txt'), (20000002, '“失控的文本”这一概念是我在研究《汉书·五行志》体例问题时产生的想法。班固以刘向《洪范五行传论》为基础, 试图纂集董仲舒《春秋》灾异说、许商《五行传记》、刘歆《洪范五行传论》等不同系统的灾异学论著, 整合成具有集大成性质的西汉灾异学总论, 从文本生成的角度而言, 《汉书·五行志》属于典型的在既有文献基础上编纂而成的“衍生型文本”, 而从其文本形态的层面来说, 一方面, 班氏未能确立一个真正可容纳诸家异说的编排体例, 另一方面, 其志文内部也存在自乱体例的现象, 两者导致最终呈现出的文本体例乖戾, 灾异事例重复、错置之处不一而足, (1) 具有鲜明的“异质性”。可以说, 《汉书·五行志》的成书形态显然未能实现\\n', 'modern', 'j_list_1.txt')]\n",
      "\n",
      "**读取的新闻数据样例：\n",
      "[(30000001, '中广网唐山６月１２日消息（记者汤一亮\\u3000庄胜春）据中国之声《新闻晚高峰》报道，今天（１２日）上午，公安机关２０１２年缉枪制爆专项行动“统一销毁非法枪爆物品活动”在河北唐山正式启动，１０万余只非法枪支、２５０余吨炸药在全国１５０个城市被统一销毁。；泼鳎合衷谖倚布，全国缉枪制爆统一销毁行动开始！Ｋ孀殴安部副部长黄明一声令下，大量仿制式枪以及猎枪、火药枪、气枪在河北唐山钢铁厂被投入炼钢炉。与此同时，在全国各省区市１５０个城市，破案追缴和群众主动上缴的１０万余支非法枪支被集中销毁，在全国各指定场所，２５０余吨炸药被分别销毁。公安部治安局局长刘绍武介绍，这次销毁的非法枪支来源于三个方面。Ａ跎芪洌捍蚧髌瓢赴括涉黑、涉恶的团伙犯罪、毒品犯罪，还有从境外非法走私的枪支爆炸物。Ｔ谙毁现场，记者看到了被追缴和上缴的各式各样的枪支。Ａ跎芪洌阂舶括制式枪，有的是军用枪、仿制的制式抢，还有猎枪、私制的火药枪等等。按照我国的枪支管理法，这些都是严厉禁止个人非法持有的。中国是世界上持枪犯罪的犯罪率最低的国家之一。Ｖ忻懒手破获特大跨国走私武器弹药案＝日，中美执法部门联手成功破获特大跨国走私武器弹药案，在中国抓获犯罪嫌疑人２３名，缴获各类枪支９３支、子弹５万余发及大量枪支配件。在美国抓获犯罪嫌疑人３名，缴获各类枪支１２支。这是公安部与美国移民海关执法局通过联合调查方式侦破重大跨国案件的又一成功案例。＃玻埃保蹦辏冈拢玻等眨上海浦东国际机场海关在对美国纽约发往浙江台州，申报品名为扩音器（音箱）的快件进行查验时，发现货物内藏有手枪９支，枪支配件９件，长枪部件７件。经检验，这些都是具有杀伤力的制式枪支及其配件。这引起了公安部和海关总署的高度重视。９安部刑侦局局长刘安成：因为是从海关进口的货物中检查出来夹带，说明来源地是境外，或是说国外，这应该是一起特大跨国走私武器弹药的案件。Ｉ虾Ｊ泄安局和上海海关缉私局成立联合专案组，迅速开展案件侦查。专案组于８月２６日在浙江台州ＵＰＳ取件处将犯罪嫌疑人王挺（男，３２岁，台州市人）抓获。王挺交代，他通过一境外网站上认识了上家林志富，２００９年１１月以来，林志富长期居住美国，他通过互联网组建了一个走私、贩卖、私藏枪支弹药的群体，通过网络在国内寻找枪支弹药买家，并通过美国ＵＰＳ联邦速递公司将枪支弹药从纽约快递给多名类似王挺的中间人，再通过中间人发送给国内买家。４税钢校犯罪分子依托虚拟网络进行犯罪交易，隐蔽性强，涉案人员使用的身份、地址、联系方式都是虚构的，侦查难度很大。刘安成说，此案体现了是新型犯罪，特别是现代犯罪的新特点。Ａ醢渤桑核不受距离的限制、经常是跨国跨境，甚至是跨一个、数个、甚至数十个国家。这种犯罪手法的改变和新型犯罪的特点，要求我们各国警方充分合作。Ｗ髡撸禾酪涣痢∽胜春\\n', 'modern', 'n_list_1.txt'), (30000002, '天津卫视求职节目《非你莫属》“晕倒门”事件余波未了，主持人张绍刚前日通过《非你莫属》节目组发出道歉信，称自己错在对留学生缺乏了解。但他的道歉，没有得到网友的接受和原谅，有网友尖锐指出，张绍刚的问题就在俯视他人，连道歉都不会。Ｕ派芨眨何沂且环好意Ｖ前哪怕被网友骂得再凶，张绍刚也表现彪悍，声称自己没错，绝不道歉。但这几天李开复领衔讨伐节目组，网上民意汹汹要他“下课”，张绍刚显然有点撑不住。前日他通过《非你莫属》节目组，发出一则语焉不详的道歉信。Ｐ胖兴将自己在节目中的表现，解释为一番好意，就像几年前程鹤麟先生说他的，“追在别人屁股后面，碎嘴叨叨地说＂你得这样，这是为了你好＂，而自己的错误，在于对求职者不够了解，没有站在别人的角度考虑问题，“当不了解一个群体的时候，就无法给出准确的判断和建议，今年以来的各种沸沸扬扬，大多源自于此。”他最后表示：“留学生的批评我很感谢，我会努力去了解这个群体的所思所想。有问题的，认识、纠正，这是咱们经常跟同学们说的，今天我对自己说！”Ｍ友：别再硬挺下去Ｕ派芨盏恼夥道歉，却没有得到网友的认可。有网友尖锐指出，张绍刚的问题在于俯视他人，连道歉都不会：“张绍刚要么在讲台，要么在舞台，对学生、对选手都掌握生杀大权，高高在上惯了，很难做到平视。他的问题不在于对哪个群体是否了解，而是他的态度。”８有网友认为，张绍刚存在自卑感，“在所有的海归学生访谈的时候，他对于相关外文的资料，先以＂我看不懂＂推脱；当学生提及在当地生活经验的时候，他就在对方的用字里面挑刺；然后吆喝批判学生数典忘祖，不爱祖国”。有网友劝张绍刚：“张先生，请先把自卑处理，再当主持人。”“别再硬挺了，再挺下去就真成棒槌了。”（余乐）Ｗ髡撸河嗬帧。ɡ丛矗貉虺峭肀ǎ\\n', 'modern', 'n_list_1.txt')]\n",
      "所有类型数据读取成功，各20000条。\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../data/database/texts.db\")\n",
    "\n",
    "print ('Opened database successfully')\n",
    "c = conn.cursor()\n",
    "\n",
    "ci_data=[]\n",
    "cursor = c.execute(\"SELECT * from ci order by id asc limit 20000\")\n",
    "for row in cursor:\n",
    "    ci_data.append(row)\n",
    "print(\"**读取的词数据样例：\")\n",
    "print(ci_data[-2:])\n",
    "\n",
    "poet_data = []\n",
    "cursor=c.execute(\"SELECT * FROM poet order by id asc limit 20000\")\n",
    "for row in cursor:\n",
    "    poet_data.append(row)\n",
    "print(\"\\n**读取的诗数据样例：\")\n",
    "print(poet_data[-2:])\n",
    "\n",
    "classical_data = []\n",
    "cursor=c.execute(\"SELECT * FROM classical order by id asc limit 20000\")\n",
    "for row in cursor:\n",
    "    classical_data.append(row)\n",
    "print(\"\\n**读取的文言文数据样例：\")\n",
    "print(classical_data[-2:])\n",
    "\n",
    "journal_data = []\n",
    "cursor=c.execute(\"SELECT * FROM journal order by id asc limit 20000\")\n",
    "for row in cursor:\n",
    "    journal_data.append(row)\n",
    "print(\"\\n**读取的期刊数据样例：\")\n",
    "print(journal_data[:2])\n",
    "    \n",
    "news_data = []\n",
    "cursor=c.execute(\"SELECT * FROM news order by id asc limit 20000\")\n",
    "for row in cursor:\n",
    "    news_data.append(row)\n",
    "print(\"\\n**读取的新闻数据样例：\")\n",
    "print(news_data[:2])\n",
    "\n",
    "print(\"所有类型数据读取成功，各20000条。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "受平台性能限制，将各类数据先做分割，之后对模型进行增量学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ci_data[:4000] + poet_data[:4000] + classical_data[:4000] + journal_data[:4000] + news_data[:4000]\n",
    "data2 = ci_data[4000:8000] + poet_data[4000:8000] + classical_data[4000:8000] + journal_data[4000:8000] + news_data[4000:8000]\n",
    "data3 = ci_data[8000:12000] + poet_data[8000:12000] + classical_data[8000:12000] + journal_data[8000:12000] + news_data[8000:12000]\n",
    "data4 = ci_data[12000:16000] + poet_data[12000:16000] + classical_data[12000:16000] + journal_data[12000:16000] + news_data[12000:16000]\n",
    "data5 = ci_data[16000:] + poet_data[16000:] + classical_data[16000:] + journal_data[16000:] + news_data[16000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅱ 数据预处理\n",
    "\n",
    "### 1. 将语篇/句子拆分为单字\n",
    "\n",
    "说明：一般而言，文本分类中提取文本特征会将文本化为词向量，因此分词是必要步骤。但是本例中，文本类型涵盖五种，其中三种为非现代文。使用目前所了解到的分词工具对于诗、词、文言文的分词效果不佳，而测试时输入的文本是未知类型的，不能够“对症下药”而只能“一视同仁”。因此本例将字作为粒度对于句子进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spaces(data): ## 输入未处理的数据，第二列为文本，返回分割好第二列的列表。\n",
    "    data_list = []\n",
    "    for line in data:\n",
    "        data_list.append(list(line))\n",
    "        \n",
    "    for item in data_list:\n",
    "        item[1] = ' '.join(list(item[1]))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 去除符号\n",
    "\n",
    "说明：降低句子中符号的干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desymbol(data):\n",
    "    import re\n",
    "    data_desymbol = []\n",
    "    for line in data:\n",
    "        data_desymbol.append(list(line))\n",
    "    for row in data_desymbol:\n",
    "        row[1] = ' '.join(list(re.sub(\"[\\s+\\.\\!\\/_,$%^*)(+\\\"\\']+|[+——！，。？：、~@#￥%……&*（）“”]\", \"\",row[1])))\n",
    "    return data_desymbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 句子头尾增加`<start>` `<end>` 标记\n",
    "\n",
    "对于部分文本，其头尾的文字是其一大特征，加入标记可以增强这一特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end(data):\n",
    "    data_with_se = []\n",
    "    for line in data:\n",
    "        data_with_se.append(list(line))\n",
    "    for row in data_with_se:\n",
    "        row[1] = \"<start> \"+row[1]+\" <end>\"\n",
    "    return data_with_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅲ 特征工程\n",
    "\n",
    "### 1. 计数向量作为特征\n",
    "\n",
    "CountVectorizer是属于常见的特征数值计算类，是一个文本特征提取方法。对于每一个训练文本，它**只考虑**每种词汇在该训练文本中出现的**频率**。\n",
    "\n",
    "CountVectorizer会将文本中的词语转换为词频矩阵，它通过fit_transform函数计算各个词语出现的次数。\n",
    "\n",
    "这里，我们认为普通的`CountVectorizer`只是对字频进行统计，没有考虑字之间的关系，因此我们修改了参数ngram_range为(1,2)*（默认为1，1）*，这样就同时用到了unigram模型和bigram模型。\n",
    "\n",
    "> I like eating apple.\n",
    ">\n",
    "> ngram_range 1,1 : (`I`) (`like`) (`eating`) (`apple`)\n",
    "> \n",
    "> ngram_range 1,2 : (`I`) (`I`,`like`) (`like`) (`like`,`eating`) (`eating`) (`eating`, `apple`) (`apple`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countV(x_train,x_test):\n",
    "    vectorizer = CountVectorizer(analyzer='char', max_features=5000,ngram_range=(1,2))\n",
    "    vectorizer.fit(x_train)\n",
    "\n",
    "    xtrain_count = vectorizer.transform(x_train)\n",
    "    xtest_count = vectorizer.transform(x_test)\n",
    "    return xtrain_count, xtest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF作为特征\n",
    "\n",
    "TF-IDF(term frequency-inverse document frequency)词频-逆向文件频率。该模型认为，字词的重要性与其在文本中出现的频率成正比(TF)，与其在语料库中出现的频率成反比(IDF)。\n",
    "\n",
    "TF为某个词在文章中出现的总次数。为了消除不同文章大小之间的差异，便于不同文章之间的比较，我们在此标准化词频：$TF = \\dfrac{某个词在文章中出现的总次数}{文章的总词数}。$\n",
    "\n",
    "IDF为逆文档频率。逆文档频率 $IDF = log \\dfrac{词料库的文档总数}{包含该词的文档数+1} $。\n",
    "\n",
    "为了避免分母为0，所以在分母上加1。\n",
    "\n",
    "$TF-IDF值 = TF * IDF。$\n",
    "\n",
    "这里我们同时也加入了ngram参数，原理同上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfIdfV(x_train,x_test):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='char', ngram_range=(1,2), max_features=5000, lowercase = False)\n",
    "    tfidf_vect_ngram.fit(x_train)\n",
    "\n",
    "    x_train_tfidf = tfidf_vect_ngram.transform(x_train)\n",
    "    x_test_tfidf = tfidf_vect_ngram.transform(x_test)\n",
    "    \n",
    "    return x_train_tfidf, x_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅳ 模型训练\n",
    "\n",
    "### 1. 朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NB(x_train,x_test,y_train,y_test): \n",
    "    model_NB = MultinomialNB()\n",
    "    model_NB.fit(x_train, y_train)\n",
    "    score = model_NB.score(x_test, y_test)\n",
    "    return model_NB,score\n",
    "\n",
    "def partial_train_NB(x_train,x_test,y_train,y_test,model_NB): # 增量训练，重复调用可用多批数据训练同一个模型\n",
    "    model_NB.partial_fit(x_train,y_train,classes=['modern','ci','classical','poet'],sample_weight=None)\n",
    "    score = score = model_NB.score(x_test, y_test)\n",
    "    return model_NB, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR(x_train,x_test,y_train,y_test):\n",
    "    model_LR = LogisticRegression()\n",
    "    model_LR.fit(x_train, y_train)\n",
    "    score = model_LR.score(x_test, y_test)\n",
    "    return model_LR, score\n",
    "    \n",
    "def partial_train_LR(x_train,x_test,y_train,y_test,model_LR):\n",
    "    model_LR.fit(x_train, y_train)\n",
    "    score = model_LR.score(x_test, y_test)\n",
    "    return model_LR, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅴ 数据实操与模型保存\n",
    "\n",
    "### 建立拆分训练、测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasplit(data_list):\n",
    "    train_data=[]\n",
    "    train_target=[]\n",
    "    for row in data_list:\n",
    "        train_data.append(row[1])\n",
    "        train_target.append(row[2])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data, train_target,test_size=0.3,random_state=0) \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数向量+朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次NB-CV综合训练得分：0.9001666666666667\n",
      "第2次NB-CV综合训练得分：0.9291666666666667\n",
      "第3次NB-CV综合训练得分：0.9223333333333333\n",
      "第4次NB-CV综合训练得分：0.8621666666666666\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "\n",
    "model_NB = MultinomialNB()\n",
    "model_NB,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB)\n",
    "print(\"第1次NB-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data2))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_NB,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB)\n",
    "print(\"第2次NB-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data3))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_NB,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB)\n",
    "print(\"第3次NB-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data4))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_NB,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB)\n",
    "print(\"第4次NB-CV综合训练得分：\"+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + 朴素贝叶斯 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次NB-TFIDF综合训练得分：0.9246666666666666\n",
      "第2次NB-TFIDF综合训练得分：0.9253333333333333\n",
      "第3次NB-TFIDF综合训练得分：0.925\n",
      "第4次NB-TFIDF综合训练得分：0.9251666666666667\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data))))\n",
    "x_train_ti , x_test_ti = tfIdfV(x_train,x_test)\n",
    "\n",
    "model_NB2 = MultinomialNB()\n",
    "model_NB2,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB2)\n",
    "print(\"第1次NB-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data2))))\n",
    "x_train_ti , x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_NB2,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB2)\n",
    "print(\"第2次NB-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data3))))\n",
    "x_train_ti , x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_NB2,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB2)\n",
    "print(\"第3次NB-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data4))))\n",
    "x_train_ti , x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_NB2,score = partial_train_NB(x_train_cv, x_test_cv, y_train, y_test, model_NB2)\n",
    "print(\"第4次NB-TFIDF综合训练得分：\"+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计数向量 + 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次LR-CV综合训练得分：0.9223333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2次LR-CV综合训练得分：0.9493333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3次LR-CV综合训练得分：0.9578333333333333\n",
      "第4次LR-CV综合训练得分：0.9426666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR,score = partial_train_LR(x_train_cv, x_test_cv, y_train, y_test, model_LR)\n",
    "print(\"第1次LR-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data2))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_LR,score = partial_train_LR(x_train_cv, x_test_cv, y_train, y_test, model_LR)\n",
    "print(\"第2次LR-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data3))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_LR,score = partial_train_LR(x_train_cv, x_test_cv, y_train, y_test, model_LR)\n",
    "print(\"第3次LR-CV综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data4))))\n",
    "x_train_cv, x_test_cv = countV(x_train,x_test)\n",
    "model_LR,score = partial_train_LR(x_train_cv, x_test_cv, y_train, y_test, model_LR)\n",
    "print(\"第4次LR-CV综合训练得分：\"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次LR-TFIDF综合训练得分：0.9271666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2次LR-TFIDF综合训练得分：0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3次LR-TFIDF综合训练得分：0.9515\n",
      "第4次LR-TFIDF综合训练得分：0.9396666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python\\python3.6\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data))))\n",
    "x_train_ti, x_test_ti = tfIdfV(x_train,x_test)\n",
    "\n",
    "model_LR2 = LogisticRegression()\n",
    "model_LR2,score = partial_train_LR(x_train_ti, x_test_ti, y_train, y_test, model_LR2)\n",
    "print(\"第1次LR-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data2))))\n",
    "x_train_ti, x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_LR2,score = partial_train_LR(x_train_ti, x_test_ti, y_train, y_test, model_LR2)\n",
    "print(\"第2次LR-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data3))))\n",
    "x_train_ti, x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_LR2,score = partial_train_LR(x_train_ti, x_test_ti, y_train, y_test, model_LR2)\n",
    "print(\"第3次LR-TFIDF综合训练得分：\"+str(score))\n",
    "\n",
    "x_train, x_test, y_train, y_test = datasplit(add_start_end(desymbol(split_spaces(data4))))\n",
    "x_train_ti, x_test_ti = tfIdfV(x_train,x_test)\n",
    "model_LR2,score = partial_train_LR(x_train_ti, x_test_ti, y_train, y_test, model_LR2)\n",
    "print(\"第4次LR-TFIDF综合训练得分：\"+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ⅵ 保存模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 models saved succesfully. see folder /model\n"
     ]
    }
   ],
   "source": [
    "file = open(\"../model/NB-CV.pickle\", \"wb\")\n",
    "pickle.dump(model_NB, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"../model/NB-TFIDF.pickle\", \"wb\")\n",
    "pickle.dump(model_NB2, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"../model/LR-CV.pickle\", \"wb\")\n",
    "pickle.dump(model_LR, file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"../model/LR-TFIDF.pickle\", \"wb\")\n",
    "pickle.dump(model_LR2, file)\n",
    "file.close()\n",
    "\n",
    "print(\"4 models saved succesfully. see folder /model\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
