{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.导入数据集预处理、特征工程和模型训练所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas,  numpy#xgboost,\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 连接数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../data/database/texts.db\")\n",
    "\n",
    "print ('Opened database successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19979\n"
     ]
    }
   ],
   "source": [
    "c = conn.cursor()\n",
    "\n",
    "ci_data=[]\n",
    "cursor = c.execute(\"SELECT * from ci\")\n",
    "for row in cursor:\n",
    "    ci_data.append(row)\n",
    "print(len(ci_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 将词、诗、文言文、期刊和新闻的数据加入data列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add successfully\n"
     ]
    }
   ],
   "source": [
    "c = conn.cursor()\n",
    "\n",
    "ci_data=[]\n",
    "cursor = c.execute(\"SELECT * from ci order by id asc limit 20000\")\n",
    "#cursor = c.execute(\"SELECT * from ci order by id\")\n",
    "for row in cursor:\n",
    "    ci_data.append(row)\n",
    "#print(\"读取的词数据样例：\")\n",
    "#print(ci_data[-2:])\n",
    "\n",
    "poet_data = []\n",
    "cursor=c.execute(\"SELECT * FROM poet order by id asc limit 4000\")\n",
    "#cursor = c.execute(\"SELECT * from poet order by id\")\n",
    "for row in cursor:\n",
    "    poet_data.append(row)\n",
    "#print(\"\\n读取的诗数据样例：\")\n",
    "#print(poet_data[-2:])\n",
    "\n",
    "classical_data = []\n",
    "cursor=c.execute(\"SELECT * FROM classical order by id asc limit 4000\")\n",
    "#cursor = c.execute(\"SELECT * from classical order by id\")\n",
    "for row in cursor:\n",
    "    classical_data.append(row)\n",
    "#print(\"\\n读取的文言文数据样例：\")\n",
    "#print(classical_data[-2:])\n",
    "\n",
    "journal_data = []\n",
    "cursor=c.execute(\"SELECT * FROM journal order by id asc limit 4000\")\n",
    "#cursor = c.execute(\"SELECT * from journal order by id\")\n",
    "for row in cursor:\n",
    "    journal_data.append(row)\n",
    "#print(\"\\n读取的期刊数据样例：\")\n",
    "#print(journal_data[:2])\n",
    "    \n",
    "news_data = []\n",
    "cursor=c.execute(\"SELECT * FROM news order by id asc limit 4000\")\n",
    "#cursor=c.execute(\"SELECT * FROM news\")\n",
    "for row in cursor:\n",
    "    news_data.append(row)\n",
    "#print(\"\\n读取的新闻数据样例：\")\n",
    "#print(news_data[:2])\n",
    "\n",
    "print(\"add successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.特征工程 + 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.1 Basic方案：对原文本拆分成单字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40002209, '中兴曾作故人看，抗节唯怜七里滩。 枯枿卧沙疑野艇，丛篁生岸忆长竿。 天边旧迹星辰动，江上余基水石寒。 应笑渭滨周吕望，白头因猎从和銮。 ', 'poet', 'd1fcb4d4-c756-418d-927d-b77c92901a98']\n",
      "[40002209, '中 兴 曾 作 故 人 看 ， 抗 节 唯 怜 七 里 滩 。   枯 枿 卧 沙 疑 野 艇 ， 丛 篁 生 岸 忆 长 竿 。   天 边 旧 迹 星 辰 动 ， 江 上 余 基 水 石 寒 。   应 笑 渭 滨 周 吕 望 ， 白 头 因 猎 从 和 銮 。  ', 'poet', 'd1fcb4d4-c756-418d-927d-b77c92901a98']\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "data = ci_data + poet_data + classical_data + journal_data + news_data\n",
    "\n",
    "data_list = []\n",
    "for line in data:\n",
    "    data_list.append(list(line))\n",
    "\n",
    "print(data_list[6000])\n",
    "for item in data_list:\n",
    "    item[1] = ' '.join(list(item[1]))\n",
    "print(data_list[6000])\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.2  建立拆分训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['气 和 玉 烛 ， 睿 化 著 鸿 明 。   缇 管 一 阳 生 。   郊 盛 礼 燔 柴 毕 ， 旋 轸 凤 凰 城 。   森 罗 仪 卫 振 华 缨 。   载 路 溢 欢 声 。   皇 图 大 业 超 前 古 ， 垂 象 泰 阶 平 。   岁 时 丰 衍 ， 九 土 乐 升 平 。   睹 寰 海 澄 清 。   道 高 尧 舜 垂 衣 治 ， 日 月 并 文 明 。   嘉 禾 甘 露 登 歌 荐 ， 云 物 焕 祥 经 。   兢 兢 惕 惕 持 谦 德 ， 未 许 禅 云 亭 。', '严 夜 警 ， 铜 莲 漏 迟 迟 。   清 禁 肃 ， 森 陛 戟 ， 羽 卫 俨 皇 闱 。   角 声 励 ， 钲 鼓 攸 宜 。   金 管 成 雅 奏 ， 逐 吹 逶 迤 。   荐 苍 璧 ， 郊 祀 神 祗 。   属 景 运 纯 禧 。   京 坻 丰 衍 ， 群 材 乐 育 ， 诸 侯 述 职 ， 盛 德 服 蛮 夷 。   殊 祥 萃 ， 九 苞 丹 凤 来 仪 。   膏 露 降 ， 和 气 洽 ， 三 秀 焕 灵 芝 。   鸿 猷 播 ， 史 册 相 辉 。   张 四 维 。   卜 世 永 固 丕 基 。   敷 玄 化 ， 荡 荡 无 为 。   合 尧 舜 文 思 。   混 并 寰 宇 ， 休 牛 归 马 ， 销 金 偃 革 ， 蹈 咏 庆 昌 期 。', '承 宝 运 ， 驯 致 隆 平 。   鸿 庆 被 寰 瀛 。   时 清 俗 阜 ， 治 定 功 成 。   遐 迩 咏 由 庚 。   严 郊 祀 ， 文 物 声 明 。   会 天 正 、 星 拱 奏 严 更 。   布 羽 仪 簪 缨 。   宸 心 虔 洁 ， 明 德 播 惟 馨 。   动 苍 冥 。   神 降 享 精 诚 。   燔 柴 半 ， 万 乘 移 天 仗 ， 肃 銮 辂 旋 衡 。   千 官 云 拥 ， 群 后 葵 倾 。   玉 帛 旅 明 庭 。   韶 荐 ， 金 奏 谐 声 。   集 休 亨 。   皇 泽 浃 黎 庶 ， 普 率 洽 恩 荣 。   仰 钦 元 后 ， 睿 圣 贯 三 灵 。   万 邦 宁 。   景 贶 福 千 龄 。']\n",
      "['modern', 'modern', 'modern']\n"
     ]
    }
   ],
   "source": [
    "train_data=[]\n",
    "train_target=[]\n",
    "for i in range(0,len(data_list)):\n",
    "    train_data.append(data_list[i][1])\n",
    "    train_target.append(data_list[i][2])\n",
    "print(train_data[:3])\n",
    "print(train_target[15000:15003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集和训练集的切分比例为0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_target,test_size=0.3,random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.3 Pro: 清理时去掉符号（利用正则表达式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除符号成功：\n",
      "样例：【中 兴 曾 作 故 人 看 ， 抗 节 唯 怜 七 里 滩 。   枯 枿 卧 沙 疑 野 艇 ， 丛 篁 生 岸 忆 长 竿 。   天 边 旧 迹 星 辰 动 ， 江 上 余 基 水 石 寒 。   应 笑 渭 滨 周 吕 望 ， 白 头 因 猎 从 和 銮 。  】=========>【中 兴 曾 作 故 人 看 抗 节 唯 怜 七 里 滩 枯 枿 卧 沙 疑 野 艇 丛 篁 生 岸 忆 长 竿 天 边 旧 迹 星 辰 动 江 上 余 基 水 石 寒 应 笑 渭 滨 周 吕 望 白 头 因 猎 从 和 銮】\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train_data_pro = []\n",
    "for paragraph in train_data:\n",
    "    train_data_pro.append(' '.join(list((re.sub(\"[\\s+\\.\\!\\/_,$%^*)(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）“”]\", \"\",paragraph)))))\n",
    "\n",
    "\n",
    "print(\"去除符号成功：\\n样例：【\"+train_data[6000]+\"】=========>【\"+train_data_pro[6000]+\"】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['客 有 通 元 先 生 好 求 古 迹 为 余 知 书 启 之 发 源 审 以 臧 否 曰 ： 余 不 敏 何 足 以 知 之 今 率 以 见 闻 随 纪 年 代 考 究 兴 亡 其 可 为 元 龟 者 举 而 叙 之 古 者 画 卦 立 象 造 字 设 教 爱 置 形 象 肇 乎 仓 史 仰 观 俯 察 鸟 迹 垂 文 至 于 唐 虞 焕 乎 文 章 畅 于 夏 殷 备 乎 秦 汉 洎 周 宣 王 史 史 籀 循 科 斗 之 书 采 仓 颉 古 文 综 其 遗 美 别 署 新 意 号 曰 籀 文 或 谓 大 篆 秦 丞 相 李 斯 改 省 籀 文 适 时 简 要 号 曰 小 篆 善 而 行 之 其 仓 颉 象 形 传 诸 典 策 世 绝 其 迹 无 得 而 称 其 籀 文 小 篆 自 周 秦 以 来 犹 如 参 用 未 之 废 黜 或 刻 以 符 玺 或 铭 于 鼎 钟 或 书 之 旌 钺 往 往 人 间 时 有 见 者 夫 言 篆 者 传 也 书 者 如 也 述 事 契 誓 者 也 字 者 孳 也 孳 乳 浸 多 者 也 而 根 之 所 由 其 来 远 矣', '侍 坐 于 君 子 君 子 欠 伸 撰 杖 履 视 日 蚤 莫 侍 坐 者 请 出 矣 侍 坐 于 君 子 君 子 问 更 端 则 起 而 对 侍 坐 于 君 子 若 有 告 者 曰 ： 少 间 愿 有 复 也 则 左 右 屏 而 待', '圉 孙 疑 作 围 亦 可 通 犯 应 从 于 作 范 钤 制 之 意 余 则 疑 圉 未 必 讹 因 圉 字 本 具 外 围 之 义 否 则 或 为 固 即 箍 之 借 总 之 无 论 为 围 为 固 都 系 名 词 似 用 金 类 制 成 故 下 文 言 融 其 两 端 若 用 木 制 不 可 融 也 融 即 镕 依 此 解 释 则 以 束 轮 三 字 文 义 自 通 不 能 依 孙 于 以 下 增 木 字 又 于 谓 围 范 即 范 围 亦 不 合 因 此 两 字 之 上 一 字 应 为 名 词 下 一 字 应 为 动 词 非 两 字 平 举 也']\n"
     ]
    }
   ],
   "source": [
    "x_train_pro, x_test_pro, y_train, y_test = train_test_split(train_data_pro, train_target,test_size=0.3,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#cv = CountVectorizer(analyzer='char')\n",
    "#vector_pro = cv.fit_transform(x_train_pro)\n",
    "#vector_pro.todense()\n",
    "vectorizer_pro = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_pro.fit(x_train_pro)\n",
    "print(x_train_pro[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"去除符号后的训练结果：\")\n",
    "train_NBayes(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_TfIdf(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_nGram(x_train_pro, x_test_pro, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0.4 Prop: 头尾加上`<start>` `<end>` 标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 中 兴 曾 作 故 人 看 抗 节 唯 怜 七 里 滩 枯 枿 卧 沙 疑 野 艇 丛 篁 生 岸 忆 长 竿 天 边 旧 迹 星 辰 动 江 上 余 基 水 石 寒 应 笑 渭 滨 周 吕 望 白 头 因 猎 从 和 銮 <end>\n"
     ]
    }
   ],
   "source": [
    "train_data_prop = []\n",
    "for paragraph in train_data_pro:\n",
    "    train_data_prop.append(\"<start> \"+paragraph+\" <end>\")\n",
    "\n",
    "print(train_data_prop[6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> １ ３ 日 福 建 省 委 书 记 孙 春 兰 在 福 州 会 见 了 日 本 驻 华 大 使 丹 羽 宇 一 郎 Ｋ 锎 豪 妓 担 丹 羽 宇 一 郎 大 使 是 中 国 人 民 的 老 朋 友 多 年 来 为 中 日 友 好 及 中 日 经 贸 往 来 作 出 了 积 极 贡 献 我 们 对 大 使 一 行 来 闽 访 问 表 示 热 烈 欢 迎 Ｔ 诩 蛞 地 介 绍 了 福 建 省 情 后 孙 春 兰 说 福 建 历 来 重 视 发 展 与 日 本 的 友 好 关 系 积 极 致 力 于 推 动 双 方 提 升 交 流 合 作 水 平 多 年 来 两 地 经 贸 往 来 频 繁 在 电 子 石 材 制 陶 食 品 加 工 旅 游 等 领 域 都 有 许 多 合 作 项 目 取 得 了 良 好 成 效 ； 交 流 交 往 密 切 福 建 与 日 本 长 崎 县 冲 绳 县 建 立 了 友 好 关 系 还 有 ６ 个 市 也 与 日 本 的 城 市 建 立 友 好 关 系 双 方 友 好 交 往 从 政 府 到 民 间 从 人 文 到 经 济 涉 及 各 个 领 域 去 年 日 本 遭 遇 大 地 震 之 时 福 建 派 出 代 表 团 到 日 本 表 示 慰 问 积 极 为 日 本 灾 后 重 建 尽 绵 薄 之 力 她 指 出 今 年 是 中 日 邦 交 正 常 化 ４ ０ 周 年 两 国 把 今 年 确 定 为 中 日 国 民 交 流 友 好 年 福 建 和 日 本 交 流 合 作 将 迎 来 难 得 机 遇 希 望 以 大 使 此 次 访 问 为 契 机 双 方 进 一 步 增 进 相 互 了 解 加 强 经 贸 合 作 促 进 相 互 投 资 深 化 友 好 交 往 共 同 推 动 两 地 友 好 关 系 不 断 向 前 发 展 ５ び 鹩 钜 焕 筛 行 话 括 福 建 人 民 在 内 的 许 多 中 国 朋 友 给 予 灾 后 重 建 的 无 私 援 助 感 谢 福 建 省 给 予 在 闽 日 本 企 业 和 日 籍 人 士 工 作 生 活 的 关 心 照 顾 他 说 今 年 是 中 日 邦 交 正 常 化 ４ ０ 周 年 两 国 将 开 展 涉 及 各 个 领 域 的 交 流 活 动 希 望 借 开 展 活 动 之 机 加 强 两 地 经 贸 文 化 体 育 等 方 方 面 面 的 交 流 合 作 进 一 步 深 化 双 方 友 好 关 系 Ｊ × 斓 家 端 瑜 倪 岳 峰 参 加 会 见 在 座 的 还 有 随 同 大 使 来 访 的 日 本 驻 广 州 领 事 馆 总 领 事 中 原 邦 之 日 本 在 华 投 资 的 企 业 代 表 记 者 兰 锋 Ｗ 髡 撸 豪 挤 妗 ɡ 丛 矗 焊 ＝ ㄈ 毡 ǎ <end>\n"
     ]
    }
   ],
   "source": [
    "x_train_prop, x_test_prop, y_train, y_test = train_test_split(train_data_prop, train_target,test_size=0.3,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "vector_prop = cv.fit_transform(x_train_prop)\n",
    "vector_prop.todense()\n",
    "vectorizer_prop = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_prop.fit(x_train_prop)\n",
    "print(x_train_prop[4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"加上标记后的训练结果：\")\n",
    "train_NBayes(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_TfIdf(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_nGram(x_train_prop, x_test_prop, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10002589, '\\t遵彼汝坟，伐其条枚。未见君子，惄如调饥。\\n', 'classical', 'c_list_10.txt'), (10002590, '\\t遵彼汝坟，伐其条肄。既见君子，不我遐弃。\\n', 'classical', 'c_list_10.txt')]\n"
     ]
    }
   ],
   "source": [
    "print(data[10000:10002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 计数向量作为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计数向量是数据集的矩阵表示，其中每行代表来自语料库的文档，每列表示来自语料库的术语，并且每个单元格表示特定文档中特定术语的频率计数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 对原始数据进行训练与预测 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', max_features=5000,ngram_range=(1,2))\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "xtrain_count = vectorizer.transform(x_train)\n",
    "xtest_count = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 对去掉符号的数据进行训练与预测 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_pro = CountVectorizer(analyzer='char', max_features=5000,ngram_range=(1,2))\n",
    "vectorizer_pro.fit(x_train_pro)\n",
    "\n",
    "xtrain_count = vectorizer_pro.transform(x_train_pro)\n",
    "xtest_count = vectorizer_pro.transform(x_test_pro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 对添加`<start>` `<end>`的数据进行训练与预测 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_prop = CountVectorizer(analyzer='char', max_features=5000, ngram_range=(1,2))\n",
    "vectorizer_prop.fit(x_train_prop)\n",
    "\n",
    "xtrain_prop_count = vectorizer_prop.transform(x_train_prop)\n",
    "xtest_prop_count = vectorizer_prop.transform(x_test_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 使用朴素贝叶斯模型训练，并得到训练分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9148333333333334"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_NB = MultinomialNB()\n",
    "#模型训练\n",
    "model_NB.fit(vectorizer.transform(x_train), y_train)\n",
    "\n",
    "#使用训练好的模型进行预测\n",
    "model_NB.score(vectorizer.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9015"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NB.fit(vectorizer_pro.transform(x_train_pro), y_train)\n",
    "model_NB.score(vectorizer_pro.transform(x_test_pro), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NB.fit(vectorizer_prop.transform(x_train_prop), y_train)\n",
    "model_NB.score(vectorizer_prop.transform(x_test_prop), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 将模型保存至本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"model_NB.pickle\", \"wb\")\n",
    "# 把模型写入到文件中\n",
    "pickle.dump(model_NB, file)\n",
    "# 关闭文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TF-IDF向量作为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF的分数代表了词语在文档和整个语料库中的相对重要性。TF-IDF分数由两部分组成：第一部分是计算标准的词语频率（TF），第二部分是逆文档频率（IDF）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。\n",
    "\n",
    "\n",
    "\n",
    "TF(t)=（该词语在文档出现的次数）/（文档中词语的总数）\n",
    "\n",
    "IDF(t)= log_e（文档总数/出现该词语的文档总数）\n",
    "\n",
    "TF-IDF向量可以由不同级别的分词产生（单个词语，词性，多个词（n-grams））\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）词语级tf-idf：矩阵代表了每个词语在不同文档中的TF-IDF分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#词语级tf-idf,使用tf-idf把文本转为向量\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='char', max_features=5000, lowercase = False)\n",
    "tfidf_vect.fit(x_train)\n",
    "#xtrain_tfidf = tfidf_vect.transform(x_train)\n",
    "#xtest_tfidf = tfidf_vect.transform(x_test)\n",
    "\n",
    "#模型训练\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(tfidf_vect.transform(x_train), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tfidf_vect.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.fit(tfidf_vect.transform(x_train_pro), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tfidf_vect.transform(x_test_pro), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier.fit(tfidf_vect.transform(x_train_prop), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tfidf_vect.transform(x_test_prop), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "file = open(\"tfidf_vect.pickle\", \"wb\")\n",
    "#把模型写入到文件中\n",
    "pickle.dump(tfidf_vect, file)\n",
    "#关闭文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "file = open(\"classifier1.pickle\", \"wb\")\n",
    "#把模型写入到文件中\n",
    "pickle.dump(classifier, file)\n",
    "#关闭文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 ngram 级tf-idf：N-grams是多个词语在一起的组合，这个矩阵代表了N-grams的TF-IDF分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.911"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram 级tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='char', ngram_range=(1,2), max_features=5000, lowercase = False)\n",
    "tfidf_vect_ngram.fit(x_train)\n",
    "\n",
    "\n",
    "#模型训练\n",
    "model_NB.fit(tfidf_vect_ngram.transform(x_train), y_train)\n",
    "#利用训练好的模型测试\n",
    "model_NB.score(tfidf_vect_ngram.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NB.fit(tfidf_vect_ngram.transform(x_train_pro), y_train)\n",
    "#利用训练好的模型测试\n",
    "model_NB.score(tfidf_vect_ngram.transform(x_test_pro), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NB.fit(tfidf_vect_ngram.transform(x_train_prop), y_train)\n",
    "#利用训练好的模型测试\n",
    "model_NB.score(tfidf_vect_ngram.transform(x_test_prop), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 保存tf-idf模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"tfidf_vect_ngram.pickle\", \"wb\")\n",
    "#把模型写入到文件中\n",
    "pickle.dump(tfidf_vect_ngram, file)\n",
    "#关闭文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）词性级tf-idf：矩阵代表了语料中多个词性的TF-IDF分数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#词性级tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(x_train)\n",
    "xtrain_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(x_train)\n",
    "xtest_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(x_test)\n",
    "\n",
    "#模型训练\n",
    "classifier.fit(xtrain_tfidf_ngram_chars, y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(xtest_tfidf_ngram_chars, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 构建线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 之前一直都是用的朴素贝叶斯模型，那下面就换一个模型玩玩哈（没错就是线性回归模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ana\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9591666666666666"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "logistic.fit(vectorizer.transform(x_train), y_train)\n",
    "logistic.score(vectorizer.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9276666666666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(vectorizer_pro.transform(x_train_pro), y_train)\n",
    "logistic.score(vectorizer_pro.transform(x_test_pro), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(vectorizer_prop.transform(x_train_prop), y_train)\n",
    "logistic.score(vectorizer_prop.transform(x_test_prop), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(tfidf_vect_ngram.transform(x_train), y_train)\n",
    "logistic.score(tfidf_vect_ngram.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9228333333333333"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(tfidf_vect_ngram.transform(x_train_pro), y_train)\n",
    "logistic.score(tfidf_vect_ngram.transform(x_test_pro), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278333333333333"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(tfidf_vect_ngram.transform(x_train_prop), y_train)\n",
    "logistic.score(tfidf_vect_ngram.transform(x_test_prop), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"logistic.pickle\", \"wb\")\n",
    "#把模型写入到文件中\n",
    "pickle.dump(logistic, file)\n",
    "#关闭文件\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "\n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors for train:  0.9591666666666666\n",
      "LR, Count Vectors for train_pro:  0.9276666666666666\n",
      "LR, Count Vectors for train_prop:  0.9266666666666666\n",
      "LR, N-Gram Vectors for train:  0.951\n",
      "LR, N-Gram Vectors for train_pro:  0.9228333333333333\n",
      "LR, N-Gram Vectors for train_prop:  0.9278333333333333\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), vectorizer.transform(x_train), y_train, vectorizer.transform(x_test))\n",
    "print(\"LR, Count Vectors for train: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), vectorizer_pro.transform(x_train_pro), y_train, vectorizer_pro.transform(x_test_pro))\n",
    "print(\"LR, Count Vectors for train_pro: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), vectorizer_prop.transform(x_train_prop), y_train, vectorizer_prop.transform(x_test_prop))\n",
    "print(\"LR, Count Vectors for train_prop: \", accuracy)\n",
    "\n",
    "#特征为多个词语级别TF-IDF向量的线性分类器\n",
    "accuracy = train_model(linear_model.LogisticRegression(), tfidf_vect_ngram.transform(x_train), y_train,tfidf_vect_ngram.transform(x_test))\n",
    "print(\"LR, N-Gram Vectors for train: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(),tfidf_vect_ngram.transform(x_train_pro), y_train,tfidf_vect_ngram.transform(x_test_pro))\n",
    "print(\"LR, N-Gram Vectors for train_pro: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), tfidf_vect_ngram.transform(x_train_prop), y_train, tfidf_vect_ngram.transform(x_test_prop))\n",
    "print(\"LR, N-Gram Vectors for train_prop: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 把朴素贝叶斯模型训练的结果一览呈现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors for train:  0.9148333333333334\n",
      "NB, Count Vectors for train:  0.9015\n",
      "LR, Count Vectors for train_prop:  0.901\n",
      "NB, N-Gram Vectors:  0.911\n",
      "LR, N-Gram Vectors for train_pro:  0.8993333333333333\n",
      "LR, N-Gram Vectors for train_prop:  0.902\n"
     ]
    }
   ],
   "source": [
    "#特征为计数向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), vectorizer.transform(x_train), y_train,vectorizer.transform(x_test))\n",
    "print(\"NB, Count Vectors for train: \", accuracy)\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), vectorizer_pro.transform(x_train_pro), y_train,vectorizer_pro.transform(x_test_pro))\n",
    "print(\"NB, Count Vectors for train: \", accuracy)\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), vectorizer_prop.transform(x_train_prop), y_train, vectorizer_prop.transform(x_test_prop))\n",
    "print(\"LR, Count Vectors for train_prop: \", accuracy)\n",
    "\n",
    "#特征为多个词语级别TF-IDF向量的朴素贝叶斯\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), tfidf_vect_ngram.transform(x_train), y_train,tfidf_vect_ngram.transform(x_test))\n",
    "print(\"NB, N-Gram Vectors: \", accuracy )\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),tfidf_vect_ngram.transform(x_train_pro), y_train,tfidf_vect_ngram.transform(x_test_pro))\n",
    "print(\"LR, N-Gram Vectors for train_pro: \", accuracy)\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), tfidf_vect_ngram.transform(x_train_prop), y_train, tfidf_vect_ngram.transform(x_test_prop))\n",
    "print(\"LR, N-Gram Vectors for train_prop: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目前仍存在的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、没有加上start和end的分字训练\n",
    "\n",
    " 因为训练这么大的数据，会很花时间，所以可以在训练的时候干别的（不用电脑内存的）\n",
    "    ---\n",
    "## 2、没有加入验证集（选做）\n",
    "## 3、还要加输入一段文本返回类别这个功能（包括输入数据的预处理）\n",
    "## 4、可视化（过程&结果&词云）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
