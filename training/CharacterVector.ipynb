{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不分词，字向量法训练分类\n",
    "\n",
    "@PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 准备工作： 连接数据库并获取各类文本数据到列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Opened database successfully**\n",
      "\n",
      "读取的词数据样例：\n",
      "[(50001999, '落日衔山，行云载雨俄鸣。 一顷新荷，坐间疑是秋声。 烟波醉客，见快哉、风恼娉婷。 香和清点，为人吹在衣襟。 珠佩欢言，放船且向前汀。 绿伞红幢，自从天汉相迎。 飞鸥独落，芦边对、几朵繁英。 侑觞人唱，乍闻应似湘灵。', 'ci', 1999), (50002000, '多幸春来云雨少。 且教月与花相照。 清色真香庭院悄。 前事杳。 还嗟此景何时了。 莫道难逢开口笑。 夜游须趁人年少。 光泛雕栏寒料峭。 迂步绕。 不劳秉烛壶天晓。  >>  词牌介绍', 'ci', 2000)]\n",
      "\n",
      "读取的诗数据样例：\n",
      "[(40001999, '朱明日盛残花卉，琼苑争游諠帝里。 宝马香车去复来，几许人心欢不已。 金明水上浮仙岛，画舸龙舟非草草。 世宁清静验如然，老者携小少随老。 匼匝烟云杨柳岸，罗绮纵横长不断。 五谷丰登顺四时，亿兆歌谣绝愁叹。 康哉阗咽芳林下，一看难醻千万价。 千平听在乐声中，比屋可封民自化。 ', 'poet', '5697307b-2273-42f3-b6fb-b4f0006e0009'), (40002000, '五粒青松护翠苔，石门岑寂断纤埃。 水浮花片知仙路，风递鸾声认啸台。 桐井晓寒千乳敛，茗园春嫩一旗开。 驰烟未勒山亭字，可是英灵许再来。 ', 'poet', 'dad91d22-4b8a-4c04-a0d5-8f7ca8aff4de')]\n",
      "\n",
      "读取的文言文数据样例：\n",
      "[(10002000, '    君子将营宫室。宗庙为先，厩库为次，居室为后。凡家造，祭器为先，牺赋 为次，养器为后。无田禄者不设祭器；有田禄者，先为祭服。君子虽贫，不粥祭 器；虽寒，不衣祭服；为宫室，不斩于丘木。\\n', 'classical', 'c_list_10.txt'), (10002001, '    大夫士去国，祭器不逾竟。大夫寓祭器于大夫，士寓祭器于士。大夫去国， 逾竟，为坛位乡国而哭。素衣、素裳、素冠、彻缘、?屦、素?蔑，乘髦马。不 蚤{髟前}。不祭食，不说人以无罪；妇人不当御。三月而复服。\\n', 'classical', 'c_list_10.txt')]\n",
      "\n",
      "读取的期刊数据样例：\n",
      "[(20000001, '作者程苏东, 北京大学中文系讲师 (北京100871) 。\\n', 'modern', 'j_list_1.txt'), (20000002, '“失控的文本”这一概念是我在研究《汉书·五行志》体例问题时产生的想法。班固以刘向《洪范五行传论》为基础, 试图纂集董仲舒《春秋》灾异说、许商《五行传记》、刘歆《洪范五行传论》等不同系统的灾异学论著, 整合成具有集大成性质的西汉灾异学总论, 从文本生成的角度而言, 《汉书·五行志》属于典型的在既有文献基础上编纂而成的“衍生型文本”, 而从其文本形态的层面来说, 一方面, 班氏未能确立一个真正可容纳诸家异说的编排体例, 另一方面, 其志文内部也存在自乱体例的现象, 两者导致最终呈现出的文本体例乖戾, 灾异事例重复、错置之处不一而足, (1) 具有鲜明的“异质性”。可以说, 《汉书·五行志》的成书形态显然未能实现\\n', 'modern', 'j_list_1.txt')]\n",
      "\n",
      "读取的新闻数据样例：\n",
      "[(30000001, '中广网唐山６月１２日消息（记者汤一亮\\u3000庄胜春）据中国之声《新闻晚高峰》报道，今天（１２日）上午，公安机关２０１２年缉枪制爆专项行动“统一销毁非法枪爆物品活动”在河北唐山正式启动，１０万余只非法枪支、２５０余吨炸药在全国１５０个城市被统一销毁。；泼鳎合衷谖倚布，全国缉枪制爆统一销毁行动开始！Ｋ孀殴安部副部长黄明一声令下，大量仿制式枪以及猎枪、火药枪、气枪在河北唐山钢铁厂被投入炼钢炉。与此同时，在全国各省区市１５０个城市，破案追缴和群众主动上缴的１０万余支非法枪支被集中销毁，在全国各指定场所，２５０余吨炸药被分别销毁。公安部治安局局长刘绍武介绍，这次销毁的非法枪支来源于三个方面。Ａ跎芪洌捍蚧髌瓢赴括涉黑、涉恶的团伙犯罪、毒品犯罪，还有从境外非法走私的枪支爆炸物。Ｔ谙毁现场，记者看到了被追缴和上缴的各式各样的枪支。Ａ跎芪洌阂舶括制式枪，有的是军用枪、仿制的制式抢，还有猎枪、私制的火药枪等等。按照我国的枪支管理法，这些都是严厉禁止个人非法持有的。中国是世界上持枪犯罪的犯罪率最低的国家之一。Ｖ忻懒手破获特大跨国走私武器弹药案＝日，中美执法部门联手成功破获特大跨国走私武器弹药案，在中国抓获犯罪嫌疑人２３名，缴获各类枪支９３支、子弹５万余发及大量枪支配件。在美国抓获犯罪嫌疑人３名，缴获各类枪支１２支。这是公安部与美国移民海关执法局通过联合调查方式侦破重大跨国案件的又一成功案例。＃玻埃保蹦辏冈拢玻等眨上海浦东国际机场海关在对美国纽约发往浙江台州，申报品名为扩音器（音箱）的快件进行查验时，发现货物内藏有手枪９支，枪支配件９件，长枪部件７件。经检验，这些都是具有杀伤力的制式枪支及其配件。这引起了公安部和海关总署的高度重视。９安部刑侦局局长刘安成：因为是从海关进口的货物中检查出来夹带，说明来源地是境外，或是说国外，这应该是一起特大跨国走私武器弹药的案件。Ｉ虾Ｊ泄安局和上海海关缉私局成立联合专案组，迅速开展案件侦查。专案组于８月２６日在浙江台州ＵＰＳ取件处将犯罪嫌疑人王挺（男，３２岁，台州市人）抓获。王挺交代，他通过一境外网站上认识了上家林志富，２００９年１１月以来，林志富长期居住美国，他通过互联网组建了一个走私、贩卖、私藏枪支弹药的群体，通过网络在国内寻找枪支弹药买家，并通过美国ＵＰＳ联邦速递公司将枪支弹药从纽约快递给多名类似王挺的中间人，再通过中间人发送给国内买家。４税钢校犯罪分子依托虚拟网络进行犯罪交易，隐蔽性强，涉案人员使用的身份、地址、联系方式都是虚构的，侦查难度很大。刘安成说，此案体现了是新型犯罪，特别是现代犯罪的新特点。Ａ醢渤桑核不受距离的限制、经常是跨国跨境，甚至是跨一个、数个、甚至数十个国家。这种犯罪手法的改变和新型犯罪的特点，要求我们各国警方充分合作。Ｗ髡撸禾酪涣痢∽胜春\\n', 'modern', 'n_list_1.txt'), (30000002, '天津卫视求职节目《非你莫属》“晕倒门”事件余波未了，主持人张绍刚前日通过《非你莫属》节目组发出道歉信，称自己错在对留学生缺乏了解。但他的道歉，没有得到网友的接受和原谅，有网友尖锐指出，张绍刚的问题就在俯视他人，连道歉都不会。Ｕ派芨眨何沂且环好意Ｖ前哪怕被网友骂得再凶，张绍刚也表现彪悍，声称自己没错，绝不道歉。但这几天李开复领衔讨伐节目组，网上民意汹汹要他“下课”，张绍刚显然有点撑不住。前日他通过《非你莫属》节目组，发出一则语焉不详的道歉信。Ｐ胖兴将自己在节目中的表现，解释为一番好意，就像几年前程鹤麟先生说他的，“追在别人屁股后面，碎嘴叨叨地说＂你得这样，这是为了你好＂，而自己的错误，在于对求职者不够了解，没有站在别人的角度考虑问题，“当不了解一个群体的时候，就无法给出准确的判断和建议，今年以来的各种沸沸扬扬，大多源自于此。”他最后表示：“留学生的批评我很感谢，我会努力去了解这个群体的所思所想。有问题的，认识、纠正，这是咱们经常跟同学们说的，今天我对自己说！”Ｍ友：别再硬挺下去Ｕ派芨盏恼夥道歉，却没有得到网友的认可。有网友尖锐指出，张绍刚的问题在于俯视他人，连道歉都不会：“张绍刚要么在讲台，要么在舞台，对学生、对选手都掌握生杀大权，高高在上惯了，很难做到平视。他的问题不在于对哪个群体是否了解，而是他的态度。”８有网友认为，张绍刚存在自卑感，“在所有的海归学生访谈的时候，他对于相关外文的资料，先以＂我看不懂＂推脱；当学生提及在当地生活经验的时候，他就在对方的用字里面挑刺；然后吆喝批判学生数典忘祖，不爱祖国”。有网友劝张绍刚：“张先生，请先把自卑处理，再当主持人。”“别再硬挺了，再挺下去就真成棒槌了。”（余乐）Ｗ髡撸河嗬帧。ɡ丛矗貉虺峭肀ǎ\\n', 'modern', 'n_list_1.txt')]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"../data/database/texts.db\")\n",
    "print ('**Opened database successfully**\\n')\n",
    "c = conn.cursor()\n",
    "ci_data=[]\n",
    "cursor = c.execute(\"SELECT * from ci order by id asc limit 2000\")\n",
    "for row in cursor:\n",
    "    ci_data.append(row)\n",
    "print(\"读取的词数据样例：\")\n",
    "print(ci_data[-2:])\n",
    "\n",
    "poet_data = []\n",
    "cursor=c.execute(\"SELECT * FROM poet order by id asc limit 2000\")\n",
    "for row in cursor:\n",
    "    poet_data.append(row)\n",
    "print(\"\\n读取的诗数据样例：\")\n",
    "print(poet_data[-2:])\n",
    "\n",
    "classical_data = []\n",
    "cursor=c.execute(\"SELECT * FROM classical order by id asc limit 2000\")\n",
    "for row in cursor:\n",
    "    classical_data.append(row)\n",
    "print(\"\\n读取的文言文数据样例：\")\n",
    "print(classical_data[-2:])\n",
    "\n",
    "journal_data = []\n",
    "cursor=c.execute(\"SELECT * FROM journal order by id asc limit 2000\")\n",
    "for row in cursor:\n",
    "    journal_data.append(row)\n",
    "print(\"\\n读取的期刊数据样例：\")\n",
    "print(journal_data[:2])\n",
    "    \n",
    "news_data = []\n",
    "cursor=c.execute(\"SELECT * FROM news order by id asc limit 2000\")\n",
    "for row in cursor:\n",
    "    news_data.append(row)\n",
    "print(\"\\n读取的新闻数据样例：\")\n",
    "print(news_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "classifier = MultinomialNB()\n",
    "tv = TfidfVectorizer(analyzer='char',max_features=5000, lowercase = False)\n",
    "\n",
    "\n",
    "def train_NBayes(x_train, x_test, y_train, y_test):\n",
    "    #模型训练\n",
    "    classifier.fit(vectorizer.transform(x_train), y_train)\n",
    "    #使用训练好的模型进行预测\n",
    "    print(\"朴素贝叶斯得分：\"+str(classifier.score(vectorizer.transform(x_test), y_test)))\n",
    "    \n",
    "def train_TfIdf(x_train, x_test, y_train, y_test):\n",
    "    tv.fit(x_train)\n",
    "    #模型训练\n",
    "    classifier.fit(tv.transform(x_train), y_train)\n",
    "    #利用训练好的模型测试\n",
    "    print(\"TF-IDF得分：\"+str(classifier.score(tv.transform(x_test), y_test)))\n",
    "def train_nGram(x_train, x_test, y_train, y_test,n):\n",
    "    #转向量\n",
    "    tv_2gram = TfidfVectorizer(analyzer='char', max_features=5000, ngram_range=(1,n),lowercase = False)\n",
    "    tv_2gram.fit(x_train)\n",
    "    #训练模型\n",
    "    clf_2gram = MultinomialNB()\n",
    "    clf_2gram.fit(tv_2gram.transform(x_train), y_train)\n",
    "    #预测\n",
    "    print(\"ngram,n为\"+str(n)+\"时得分：\"+str(clf_2gram.score(tv_2gram.transform(x_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⅠBasic方案：对原文本拆分成单字\n",
    "### 1. 暴力拆字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000001, '作者程苏东, 北京大学中文系讲师 (北京100871) 。\\n', 'modern', 'j_list_1.txt']\n",
      "[20000001, '作 者 程 苏 东 ,   北 京 大 学 中 文 系 讲 师   ( 北 京 1 0 0 8 7 1 )   。 \\n', 'modern', 'j_list_1.txt']\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "data = ci_data + poet_data + classical_data + journal_data + news_data\n",
    "data_list = []\n",
    "for line in data:\n",
    "    data_list.append(list(line))\n",
    "\n",
    "print(data_list[6000])\n",
    "for item in data_list:\n",
    "    item[1] = ' '.join(list(item[1]))\n",
    "print(data_list[6000])\n",
    "print(len(data_list))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 建立训练集、测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "train_target=[]\n",
    "for i in range(0,10000):\n",
    "    train_data.append(data_list[i][1])\n",
    "    train_target.append(data_list[i][2])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data, train_target,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 153,    0,    0, ...,    0,    0,    0],\n",
       "        [  13,    0,    0, ...,    0,    0,    0],\n",
       "        [ 128,    0,    0, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  32,    0,    0, ...,    0,    0,    0],\n",
       "        [1085,    0,    0, ...,    0,    0,    0],\n",
       "        [  12,    0,    0, ...,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "vector = cv.fit_transform(x_train)\n",
    "vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['。 因 此 ,   基 层 治 理 能 力 制 度 建 设 也 与 村 庄 社 会 基 础 有 密 切 的 关 系 。 杨 念 群 认 为 ,   与 城 市 中 的 社 会 生 活 相 比 ,   农 村 社 会 的 程 式 化 和 模 式 化 程 度 是 很 低 的 ,   实 际 上 缺 少 一 成 不 变 的 正 式 程 序 和 正 式 规 则 。 在 许 多 情 况 下 ,   即 使 存 在 这 样 的 程 序 和 规 则 ,   有 时 也 不 会 真 正 起 作 用 ; 相 反 ,   一 些 重 要 而 敏 感 问 题 的 解 决 ,   往 往 采 取 非 正 式 的 方 式 或 随 机 处 理 的 弹 性 手 段 \\n', '\\u3000 \\u3000 众 趋 明 所 避 ， 时 弃 道 犹 存 。   \\n', '桐 轩 潇 洒 远 尘 机 ， 伯 始 英 风 世 不 衰 。   千 亩 自 封 轻 渭 叟 ， 五 经 同 拜 重 宣 尼 。   衣 无 常 主 门 多 义 ， 仓 有 余 粮 俗 共 熙 。   仁 智 眼 前 从 所 乐 ， 利 名 身 外 更 何 思 。   琴 清 善 召 云 中 鹤 ， 床 稳 高 蓍 叶 上 龟 。   茶 煮 玉 泉 僧 至 日 ， 酒 𫇴 金 蚁 菊 芳 时 。   往 来 虚 席 宾 朋 盛 ， 朝 暮 过 堂 子 弟 奇 。   万 卷 诗 书 归 腹 箧 ， 地 仙 踪 迹 少 人 知 。  ']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer.fit(x_train)\n",
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "暴力拆字方法的训练结果：\n",
      "朴素贝叶斯得分：0.87575\n",
      "TF-IDF得分：0.8175\n",
      "ngram,n为2时得分：0.879\n"
     ]
    }
   ],
   "source": [
    "print(\"暴力拆字方法的训练结果：\")\n",
    "train_NBayes(x_train, x_test, y_train, y_test)\n",
    "train_TfIdf(x_train, x_test, y_train, y_test)\n",
    "train_nGram(x_train, x_test, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro: 清理时去掉符号（利用正则表达式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除符号成功：\n",
      "样例：【作 者 程 苏 东 ,   北 京 大 学 中 文 系 讲 师   ( 北 京 1 0 0 8 7 1 )   。 \n",
      "】=========>【作 者 程 苏 东 北 京 大 学 中 文 系 讲 师 北 京 1 0 0 8 7 1】\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train_data_pro = []\n",
    "for paragraph in train_data:\n",
    "    train_data_pro.append(' '.join(list((re.sub(\"[\\s+\\.\\!\\/_,$%^*)(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]\", \"\",paragraph)))))\n",
    "\n",
    "\n",
    "print(\"去除符号成功：\\n样例：【\"+train_data[6000]+\"】=========>【\"+train_data_pro[6000]+\"】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['因 此 基 层 治 理 能 力 制 度 建 设 也 与 村 庄 社 会 基 础 有 密 切 的 关 系 杨 念 群 认 为 与 城 市 中 的 社 会 生 活 相 比 农 村 社 会 的 程 式 化 和 模 式 化 程 度 是 很 低 的 实 际 上 缺 少 一 成 不 变 的 正 式 程 序 和 正 式 规 则 在 许 多 情 况 下 即 使 存 在 这 样 的 程 序 和 规 则 有 时 也 不 会 真 正 起 作 用 ; 相 反 一 些 重 要 而 敏 感 问 题 的 解 决 往 往 采 取 非 正 式 的 方 式 或 随 机 处 理 的 弹 性 手 段', '众 趋 明 所 避 时 弃 道 犹 存', '桐 轩 潇 洒 远 尘 机 伯 始 英 风 世 不 衰 千 亩 自 封 轻 渭 叟 五 经 同 拜 重 宣 尼 衣 无 常 主 门 多 义 仓 有 余 粮 俗 共 熙 仁 智 眼 前 从 所 乐 利 名 身 外 更 何 思 琴 清 善 召 云 中 鹤 床 稳 高 蓍 叶 上 龟 茶 煮 玉 泉 僧 至 日 酒 𫇴 金 蚁 菊 芳 时 往 来 虚 席 宾 朋 盛 朝 暮 过 堂 子 弟 奇 万 卷 诗 书 归 腹 箧 地 仙 踪 迹 少 人 知']\n"
     ]
    }
   ],
   "source": [
    "x_train_pro, x_test_pro, y_train, y_test = train_test_split(train_data_pro, train_target,test_size=0.4,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "vector_pro = cv.fit_transform(x_train_pro)\n",
    "vector_pro.todense()\n",
    "vectorizer_pro = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_pro.fit(x_train_pro)\n",
    "print(x_train_pro[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除符号后的训练结果：\n",
      "朴素贝叶斯得分：0.87075\n",
      "TF-IDF得分：0.8285\n",
      "ngram,n为2时得分：0.87325\n"
     ]
    }
   ],
   "source": [
    "print(\"去除符号后的训练结果：\")\n",
    "train_NBayes(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_TfIdf(x_train_pro, x_test_pro, y_train, y_test)\n",
    "train_nGram(x_train_pro, x_test_pro, y_train, y_test,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro+: 头尾加上`<start>` `<end>` 标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 作 者 程 苏 东 北 京 大 学 中 文 系 讲 师 北 京 1 0 0 8 7 1 <end>\n"
     ]
    }
   ],
   "source": [
    "train_data_prop = []\n",
    "for paragraph in train_data_pro:\n",
    "    train_data_prop.append(\"<start> \"+paragraph+\" <end>\")\n",
    "\n",
    "print(train_data_prop[6000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> 淡 云 残 雪 簇 江 天 策 蹇 迟 回 客 兴 阑 持 钵 老 僧 来 咒 水 倚 船 商 女 待 搬 滩 沙 翘 白 鹭 非 真 静 竹 映 繁 梅 奈 苦 寒 阮 籍 莫 嗟 歧 路 异 旧 山 溪 畔 有 渔 竿 <end>\n"
     ]
    }
   ],
   "source": [
    "x_train_prop, x_test_prop, y_train, y_test = train_test_split(train_data_prop, train_target,test_size=0.4,random_state=0)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "vector_prop = cv.fit_transform(x_train_prop)\n",
    "vector_prop.todense()\n",
    "vectorizer_prop = CountVectorizer(analyzer='char', max_features=5000)\n",
    "vectorizer_prop.fit(x_train_prop)\n",
    "print(x_train_prop[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加上标记后的训练结果：\n",
      "朴素贝叶斯得分：0.85975\n",
      "TF-IDF得分：0.83775\n",
      "ngram,n为2时得分：0.883\n"
     ]
    }
   ],
   "source": [
    "print(\"加上标记后的训练结果：\")\n",
    "train_NBayes(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_TfIdf(x_train_prop, x_test_prop, y_train, y_test)\n",
    "train_nGram(x_train_prop, x_test_prop, y_train, y_test,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
